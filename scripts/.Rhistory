#getting MLEs
optimize <- optim(beta0, loglike, X = X, Y = Y, method = "BFGS", hessian = TRUE)
print(optimize$par)
print(optimize$convergence)
sqrt(diag(solve(optimize$hessian)))
sqrt(diag(solve(-optimize$hessian)))
#libraries
library(tidyverse)
#reading in new dataset
path <- '/Users/adarekar/Documents/College/Senior/ST 440/Final Project/short_data.csv'
data <- read_csv(file = path)
data <- select(data, -c("year", "officer_id_hash"))
#creating logistic model
Y <- as.matrix(data$arrest)
X <- as.matrix(select(data, -c("arrest")))
#adding intercept
X <- cbind(1, X)
#libraries
library(tidyverse)
#reading in new dataset
path <- '/Users/adarekar/Documents/College/Senior/ST 440/Final Project/short_data.csv'
data <- read_csv(file = path)
#libraries
library(tidyverse)
#reading in new dataset
path <- '/Users/adarekar/Documents/College/Senior/ST 440/Final Project/short_data.csv'
data <- read_csv(file = path)
data <- select(data, -c("year", "officer_id_hash"))
#creating logistic model
Y <- as.matrix(data$arrest)
X <- as.matrix(select(data, -c("arrest")))
#adding intercept
X <- cbind(1, X)
#function that defines negative of log likelihood
loglike <- function(beta, X, Y) {
return(-sum(Y*(X %*% beta - log(1+exp(X %*% beta)))+
(1-Y)*(-log(1+exp(X %*% beta)))))
}
#initial values for betas
beta0 <- rep(0, ncol(X))
#getting MLEs
optimize <- optim(beta0, loglike, X = X, Y = Y, method = "BFGS", hessian = TRUE)
print(optimize$par)
print(optimize$convergence)
print(sqrt(diag(solve(optimize$hessian))))
#AIC calculation
aic <- -2*optimize$value + 2*length(beta0)
aic
#libraries
library(tidyverse)
#reading in new dataset
path <- '/Users/adarekar/Documents/College/Senior/ST 440/Final Project/short_data.csv'
data <- read_csv(file = path)
data <- select(data, -c("year", "officer_id_hash"))
#creating logistic model
Y <- as.matrix(data$arrest)
X <- as.matrix(select(data, -c("arrest")))
#adding intercept
X <- cbind(1, X)
#function that defines negative of log likelihood
loglike <- function(beta, X, Y) {
return(-sum(Y*(X %*% beta - log(1+exp(X %*% beta)))+
(1-Y)*(-log(1+exp(X %*% beta)))))
}
#initial values for betas
beta0 <- colMeans(X)
beta0
#getting MLEs
optimize <- optim(beta0, loglike, X = X, Y = Y, method = "BFGS", hessian = TRUE)
print(optimize$par)
print(optimize$convergence)
#AIC calculation
aic <- -2*optimize$value + 2*length(beta0)
aic
View(data)
test(optimize$par[2], sqrt(diag(solve(optimize$hessian)))[2])
#hypothesis testing
test <- function(mean, sd) {
samples <- rnorm(10000, mean, sd)
print(mean(samples < 0))
}
test(optimize$par[2], sqrt(diag(solve(optimize$hessian)))[2])
optimize$par[2]
sqrt(diag(solve(optimize$hessian)))[2]
#libraries
library(tidyverse)
#reading in new dataset
path <- '/Users/adarekar/Documents/College/Senior/ST 440/Final Project/short_data.csv'
data <- read_csv(file = path)
data <- select(data, -c("year", "officer_id_hash"))
#creating logistic model
Y <- as.matrix(data$arrest)
X <- as.matrix(select(data, -c("arrest")))
#adding intercept
X <- cbind(1, X)
#function that defines negative of log likelihood
loglike <- function(beta, X, Y) {
return(-sum(Y*(X %*% beta - log(1+exp(X %*% beta)))+
(1-Y)*(-log(1+exp(X %*% beta)))))
}
#initial values for betas
beta0 <- rep(0, ncol(X))
#getting MLEs
optimize <- optim(beta0, loglike, X = X, Y = Y, method = "BFGS", hessian = TRUE)
#hypothesis testing
credinterval <- function(mean, sd) {
samples <- rnorm(10000, mean, sd)
mean_s <- mean(samples)
sd_s <- sd(samples)
print(qnorm(c(0.025, 0.975)), mean_s, sd_s)
}
credinterval(optimize$par[2], sqrt(diag(solve(optimize$hessian)))[2])
#hypothesis testing
credinterval <- function(mean, sd) {
samples <- rnorm(10000, mean, sd)
mean_s <- mean(samples)
sd_s <- sd(samples)
print(mean_s)
print(sd_s)
print(qnorm(c(0.025, 0.975)), mean_s, sd_s)
}
credinterval(optimize$par[2], sqrt(diag(solve(optimize$hessian)))[2])
#hypothesis testing
credinterval <- function(mean, sd) {
samples <- rnorm(10000, mean, sd)
mean_s <- mean(samples)
sd_s <- sd(samples)
print(mean_s)
print(sd_s)
print(qnorm(c(0.025, 0.975), mean_s, sd_s))
}
credinterval(optimize$par[2], sqrt(diag(solve(optimize$hessian)))[2])
#hypothesis testing
credinterval <- function(mean, sd) {
samples <- rnorm(10000, mean, sd)
mean_s <- mean(samples)
sd_s <- sd(samples)
print(qnorm(c(0.025, 0.975), mean_s, sd_s))
}
credinterval(optimize$par[2], sqrt(diag(solve(optimize$hessian)))[2])
View(data)
#hypothesis testing
credinterval <- function(mean, sd) {
samples <- rnorm(10000, mean, sd)
mean_s <- mean(samples)
sd_s <- sd(samples)
print(exp(qnorm(c(0.025, 0.975), mean_s, sd_s)))
}
credinterval(optimize$par[2], sqrt(diag(solve(optimize$hessian)))[2])
exp(optimize$par[2])
print(optimize$par)
exp(optimize$par)
#reading in new dataset
path <- '/Users/adarekar/Documents/College/Senior/ST 440/Final Project/full_data.csv'
#libraries
library(tidyverse)
#reading in cleaned dataset
path <- '/Users/adarekar/Documents/College/Senior/ST 440/Final Project/full_data.csv'
full_data <- read_csv(file = path)
#libraries
library(tidyverse)
#reading in cleaned dataset
path <- '/Users/adarekar/Documents/College/Senior/ST 440/Final Project/full_data.csv'
#getting design and response datasets
full_data <- read_csv(file = path)
X <- full_data |>
select(-year, -officer_id_hash, -arrest, -subject_age,
-contains(c("search", "stop")), -frisk_performed,
-subject_race_hispanic) |>
add_column(1, .before = "std_subject_age") |>
rename("intercept" = `1`) |> as.matrix()
Y <- full_data$arrest
#function that defines negative of log likelihood
loglike <- function(beta, X, Y) {
return(-sum(Y*(X %*% beta - log(1+exp(X %*% beta)))+
(1-Y)*(-log(1+exp(X %*% beta)))))
}
#initial values for betas
beta0 <- rep(0, ncol(X))
#getting MLEs
optimize <- optim(beta0, loglike, X = X, Y = Y, method = "BFGS", hessian = TRUE)
print(optimize$par)
print(optimize$convergence)
print(sqrt(diag(solve(optimize$hessian)))) #standard deviations
#AIC calculation
aic <- -2*optimize$value + 2*length(beta0)
print(aic)
View(X)
#calculate credible interval
credinterval <- function(mean, sd) {
samples <- rnorm(10000, mean, sd)
mean_s <- mean(samples)
sd_s <- sd(samples)
print(qnorm(c(0.025, 0.975), mean_s, sd_s))
}
View(X)
beta_est <- optimize$par
print(beta_est)
names(X)
colnames(X)
names(beta_est)
beta_est <- optimize$par
names(beta_est) <- colnames(X)
beta_est
beta_std <- sqrt(diag(solve(optimize$hessian)))
names(beta_std) <- colnames(X)
beta_est
beta_std
#CI for subject_male beta
credinterval(optimize$par[4], sqrt(diag(solve(optimize$hessian)))[4])
#CI for subject_hispanic - was this considered a race?
credinterval(optimize$par[5], sqrt(diag(solve(optimize$hessian)))[5])
#CI for subject_race_asian_pacific_islander
credinterval(optimize$par[6], sqrt(diag(solve(optimize$hessian)))[6])
#CI for subject_race_black
credinterval(optimize$par[7], sqrt(diag(solve(optimize$hessian)))[7])
#CI for subject_race_other
credinterval(optimize$par[8], sqrt(diag(solve(optimize$hessian)))[8])
#CI for subject_race_unknown
credinterval(optimize$par[9], sqrt(diag(solve(optimize$hessian)))[9])
#including year
Xyear <- full_data |>
select( -officer_id_hash, -arrest, -subject_age,
-contains(c("search", "stop")), -frisk_performed,
-subject_race_hispanic) |>
add_column(1, .before = "year") |>
rename("intercept" = `1`) |> as.matrix()
ncols(Xyear)
View(Xyear)
View(X)
#including year
Xyear <- full_data |>
select( -officer_id_hash, -arrest, -subject_age,
-contains(c("search", "stop")), -frisk_performed,
-subject_race_hispanic) |>
add_column(1, .before = "year") |>
rename("intercept" = `1`) |> as.matrix()
Yyear <- full_data$arrest
#getting MLEs
initials <- rep(0,ncol(Xyear))
#getting MLEs
beta0 <- rep(0,ncol(Xyear))
#including year
Xyear <- full_data |>
select( -officer_id_hash, -arrest, -subject_age,
-contains(c("search", "stop")), -frisk_performed,
-subject_race_hispanic) |>
add_column(1, .before = "year") |>
rename("intercept" = `1`) |> as.matrix()
Yyear <- full_data$arrest
#getting MLEs
beta0 <- rep(0,ncol(Xyear))
optimize_yr <- optim(beta0, loglike, X = Xyear, Y = Yyear, method = "BFGS", hessian = TRUE)
beta_est <- optimize_yr$par
names(beta_est) <- colnames(Xyear)
beta_std <- sqrt(diag(solve(optimize_yr$hessian)))
names(beta_std) <- colnames(Xyear)
beta_est
#reading in cleaned dataset
path <- '/Users/adarekar/Documents/College/Senior/ST 440/Final Project/full_data.csv'
#getting design and response datasets
full_data <- read_csv(file = path)
X <- full_data |>
select(-year, -officer_id_hash, -arrest, -subject_age,
-contains(c("search", "stop")), -frisk_performed,
-subject_race_hispanic) |>
add_column(1, .before = "std_subject_age") |>
rename("intercept" = `1`) |> as.matrix()
Y <- full_data$arrest
#function that defines negative of log likelihood
loglike <- function(beta, X, Y) {
return(-sum(Y*(X %*% beta - log(1+exp(X %*% beta)))+
(1-Y)*(-log(1+exp(X %*% beta)))))
}
#initial values for betas
beta0 <- rep(0, ncol(X))
#getting MLEs
optimize <- optim(beta0, loglike, X = X, Y = Y, method = "BFGS", hessian = TRUE)
print(optimize$convergence)
#AIC calculation
aic <- -2*optimize$value + 2*length(beta0)
print(aic)
#calculate credible interval
credinterval <- function(mean, sd) {
samples <- rnorm(10000, mean, sd)
mean_s <- mean(samples)
sd_s <- sd(samples)
print(qnorm(c(0.025, 0.975), mean_s, sd_s))
}
beta_est <- optimize$par
names(beta_est) <- colnames(X)
beta_std <- sqrt(diag(solve(optimize$hessian)))
names(beta_std) <- colnames(X)
#CI for subject_male beta
credinterval(beta_est[4], beta_std[4])
#CI for subject_hispanic - was this considered a race?
credinterval(beta_est[5], beta_std[5])
#CI for subject_race_asian_pacific_islander
credinterval(beta_est[6], beta_std[6])
#CI for subject_race_black
credinterval(beta_est[7], beta_std[7])
#CI for subject_race_other
credinterval(beta_est[8], beta_std[8])
#CI for subject_race_unknown
credinterval(beta_est[9], beta_std[9])
#including year
Xyear <- full_data |>
select( -officer_id_hash, -arrest, -subject_age,
-contains(c("search", "stop")), -frisk_performed,
-subject_race_hispanic) |>
add_column(1, .before = "year") |>
rename("intercept" = `1`) |> as.matrix()
Yyear <- full_data$arrest
#getting MLEs
beta0 <- rep(0,ncol(Xyear))
optimize_yr <- optim(beta0, loglike, X = Xyear, Y = Yyear, method = "BFGS", hessian = TRUE)
beta_est <- optimize_yr$par
print(optimize_yr$convergence)
beta_est <- optimize_yr$par
names(beta_est) <- colnames(Xyear)
beta_std <- sqrt(diag(solve(optimize_yr$hessian)))
names(beta_std) <- colnames(Xyear)
beta_std
beta_est
#CI for year
credinterval(beta_est[2], beta_std[2])
#libraries
library(tidyverse)
library(ranger)
library(MASS)
library(pROC)
library(rstudioapi)
#change working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#read in data
path <- '../data/processed_data/'
data <- read_csv(paste(path, 'cleaned-maude-2016-2019.csv', sep = ""))
#preprocessing
data <- as.data.frame(data)
data <- na.omit(data)
data <- subset(data, select = -c(...1, PATIENT_PROBLEM_DESCRIPTION, DEVICE_PROBLEM_DESCRIPTION, MDR_REPORT_KEY, DEVICE_PROBLEM_CODE, PATIENT_PROBLEM_CODE, REPORTER_OCCUPATION_CODE, DEVICE_REPORT_PRODUCT_CODE, REPORTER_COUNTRY_CODE))
data$EVENT_TYPE <- as.factor(data$EVENT_TYPE)
#cross validation 70%/30% split
K<-5
n<-nrow(data)
train.prop <-0.7
train.size <-ceiling(n*train.prop) #integer above number
rf1.err.cv <-rep(NA, K)
rf2.err.cv <-rep(NA, K)
rf3.err.cv <-rep(NA, K)
for (i in 1:K) {
set.seed(234)
indices <- sample(n, train.size)
train.data <- data[indices,]
test.data <- data[-indices,]
tree1 <- ranger(EVENT_TYPE~., data = train.data, mtry = 3, num.trees = 50, importance = "impurity")
tree2 <- ranger(EVENT_TYPE~., data = train.data, mtry = 3, num.trees = 100, importance = "impurity")
tree3 <- ranger(EVENT_TYPE~., data = train.data, mtry = 3, num.trees = 200, importance = "impurity")
pred.tree1 <- predict(tree1, data = subset(test.data, select = -c(EVENT_TYPE)))
pred.tree2 <- predict(tree2, data = subset(test.data, select = -c(EVENT_TYPE)))
pred.tree3 <- predict(tree3, data = subset(test.data, select = -c(EVENT_TYPE)))
rf1.err.cv[i] <- sum(pred.tree1$predictions != test.data$EVENT_TYPE)/nrow(test.data)
rf2.err.cv[i] <- sum(pred.tree2$predictions != test.data$EVENT_TYPE)/nrow(test.data)
rf3.err.cv[i] <- sum(pred.tree3$predictions != test.data$EVENT_TYPE)/nrow(test.data)
}
mean(rf1.err.cv)
mean(rf2.err.cv)
mean(rf3.err.cv)
#go with tree 3 (mtry = 3, num.trees = 200) with 70%/30% split
set.seed(234)
train <- sample(n, ceiling(0.7*n))
train_data <- data[train,]
test <- data[-train,]
#training and evaluating tree
tree <- ranger(EVENT_TYPE~., data = train_data, mtry = 3, num.trees = 200, importance = "impurity")
pred.tree <- predict(tree, data = subset(test, select = -c(EVENT_TYPE)))
table(test$EVENT_TYPE, pred.tree$predictions)
sum(pred.tree$predictions != test$EVENT_TYPE)/nrow(test)
#roc curve
roc_score<-multiclass.roc(pred.tree$predictions, as.numeric(test$EVENT_TYPE))
plot.roc(roc_score$rocs[[1]], col = 'green',
print.auc=T,
legacy.axes = T)
plot.roc(roc_score$rocs[[2]],
add=T, col = 'red',
print.auc = T,
legacy.axes = T,
print.auc.adj = c(0,3))
plot.roc(roc_score$rocs[[3]],add=T, col = 'blue',
print.auc=T,
legacy.axes = T,
print.auc.adj = c(0,5))
legend('bottomright',
legend = c('Death-Injury',
'Death-Malfunction',
'Injury-Malfunction'),
col=c('green', 'red', 'blue'),lwd=2)
#distribution of response to show uneven distribution
table(data$EVENT_TYPE)
#read in libraries
library(tidyverse)
library(rstudioapi)
#change working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#read in datasets
path <- '../data/raw_data/'
device_data <- read_delim(paste(path, 'DEVICE2016.txt', sep = ""), delim = '|', col_names = TRUE)
patient_data <- read_csv(paste(path, 'patient2016.csv', sep = ""))
mdr_data <- read_csv(paste(path, 'mdr2016.csv', sep = ""))
patient_problem <- read_delim(paste(path, 'patientproblemcode.txt', sep =""), delim = '|', col_names = TRUE)
foi_dev_problem <- read_delim(paste(path, 'foidevproblem.txt', sep = ""), delim = '|', col_names = c('MDR_REPORT_KEY', 'DEVICE_PROBLEM_CODE'))
device_prob_code <- read_csv(paste(path, 'deviceproblemcodes.csv', sep = ""), col_names = c('DEVICE_PROBLEM_CODE', 'PROBLEM_DESCRIPTION'))
patient_prob_code <- read_csv(paste(path, 'patientproblemcodes.csv', sep = ""), col_names = c('PATIENT_PROBLEM_CODE', 'PROBLEM_DESCRIPTION'))
#convert mdr report key variable in patient problem code to integer
patient_problem$MDR_REPORT_KEY <- as.numeric(patient_problem$MDR_REPORT_KEY)
#only keep variables of interest
device_data <- select(device_data, MDR_REPORT_KEY, MANUFACTURER_D_NAME,DEVICE_REPORT_PRODUCT_CODE,
COMBINATION_PRODUCT_FLAG, BRAND_NAME)
#only keep variables of interest
device_data <- select(device_data, MDR_REPORT_KEY, MANUFACTURER_D_NAME,DEVICE_REPORT_PRODUCT_CODE,
COMBINATION_PRODUCT_FLAG, BRAND_NAME)
#read in libraries
library(tidyverse)
library(dplyr)
library(rstudioapi)
#change working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#read in datasets
path <- '../data/raw_data/'
device_data <- read_delim(paste(path, 'DEVICE2016.txt', sep = ""), delim = '|', col_names = TRUE)
patient_data <- read_csv(paste(path, 'patient2016.csv', sep = ""))
mdr_data <- read_csv(paste(path, 'mdr2016.csv', sep = ""))
patient_problem <- read_delim(paste(path, 'patientproblemcode.txt', sep =""), delim = '|', col_names = TRUE)
foi_dev_problem <- read_delim(paste(path, 'foidevproblem.txt', sep = ""), delim = '|', col_names = c('MDR_REPORT_KEY', 'DEVICE_PROBLEM_CODE'))
device_prob_code <- read_csv(paste(path, 'deviceproblemcodes.csv', sep = ""), col_names = c('DEVICE_PROBLEM_CODE', 'PROBLEM_DESCRIPTION'))
patient_prob_code <- read_csv(paste(path, 'patientproblemcodes.csv', sep = ""), col_names = c('PATIENT_PROBLEM_CODE', 'PROBLEM_DESCRIPTION'))
#convert mdr report key variable in patient problem code to integer
patient_problem$MDR_REPORT_KEY <- as.numeric(patient_problem$MDR_REPORT_KEY)
#only keep variables of interest
device_data <- select(device_data, MDR_REPORT_KEY, MANUFACTURER_D_NAME,DEVICE_REPORT_PRODUCT_CODE,
COMBINATION_PRODUCT_FLAG, BRAND_NAME)
library(tidyverse)
library(dplyr)
library(rstudioapi)
#change working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#read in datasets
path <- '../data/raw_data/'
device_data <- read_delim(paste(path, 'DEVICE2016.txt', sep = ""), delim = '|', col_names = TRUE)
patient_data <- read_csv(paste(path, 'patient2016.csv', sep = ""))
mdr_data <- read_csv(paste(path, 'mdr2016.csv', sep = ""))
patient_problem <- read_delim(paste(path, 'patientproblemcode.txt', sep =""), delim = '|', col_names = TRUE)
foi_dev_problem <- read_delim(paste(path, 'foidevproblem.txt', sep = ""), delim = '|', col_names = c('MDR_REPORT_KEY', 'DEVICE_PROBLEM_CODE'))
device_prob_code <- read_csv(paste(path, 'deviceproblemcodes.csv', sep = ""), col_names = c('DEVICE_PROBLEM_CODE', 'PROBLEM_DESCRIPTION'))
patient_prob_code <- read_csv(paste(path, 'patientproblemcodes.csv', sep = ""), col_names = c('PATIENT_PROBLEM_CODE', 'PROBLEM_DESCRIPTION'))
#convert mdr report key variable in patient problem code to integer
patient_problem$MDR_REPORT_KEY <- as.numeric(patient_problem$MDR_REPORT_KEY)
#only keep variables of interest
device_data <- dplyr::select(device_data, MDR_REPORT_KEY, MANUFACTURER_D_NAME,DEVICE_REPORT_PRODUCT_CODE,
COMBINATION_PRODUCT_FLAG, BRAND_NAME)
patient_data <- dplyr::select(patient_data, MDR_REPORT_KEY, DATE_RECEIVED)
mdr_data <- dplyr::select(mdr_data, MDR_REPORT_KEY, REPORT_SOURCE_CODE, DATE_RECEIVED, DATE_OF_EVENT, REPORTER_OCCUPATION_CODE,
EVENT_TYPE, MANUFACTURER_NAME, REPORTER_COUNTRY_CODE, PMA_PMN_NUM)
patient_problem <- dplyr::select(patient_problem, MDR_REPORT_KEY, PROBLEM_CODE)
#merge datasets using mdr report key and date_received variable
merge_data <- merge(mdr_data, patient_data, by = c("MDR_REPORT_KEY", "DATE_RECEIVED"))
merge_data <- merge(merge_data, device_data, by = c("MDR_REPORT_KEY"))
merge_data <- merge(merge_data, foi_dev_problem,  by = c("MDR_REPORT_KEY"))
merge_data <- merge(merge_data, patient_problem, by = c("MDR_REPORT_KEY"))
#drop MANUFACTURER_NAME
merge_data <- select(merge_data, -MANUFACTURER_NAME)
#drop MANUFACTURER_NAME
merge_data <- dplyr::select(merge_data, -MANUFACTURER_NAME)
#match patient and device codes with their description
p_prob_descrip <- rep(NA, length(merge_data$PROBLEM_CODE))
d_prob_descrip <- rep(NA, length(merge_data$DEVICE_PROBLEM_CODE))
p_code <- merge_data$PROBLEM_CODE
d_code <- merge_data$DEVICE_PROBLEM_CODE
for (i in 1:length(p_code)) {
index_d <- which(device_prob_code$DEVICE_PROBLEM_CODE == d_code[i])
d_prob_descrip[i] <- device_prob_code$PROBLEM_DESCRIPTION[index_d]
index_p <- which(patient_prob_code$PATIENT_PROBLEM_CODE == p_code[i])
p_prob_descrip[i] <- patient_prob_code$PROBLEM_DESCRIPTION[index_p]
}
#append descriptions to merge_data
merge_data$PATIENT_PROBLEM_DESCRIPTION <- p_prob_descrip
merge_data$DEVICE_PROBLEM_DESCRIPTION <- d_prob_descrip
library(tidyverse)
library(rstudioapi)
#change working directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
#read in datasets
path <- '../data/processed_data/'
data2016 <- read_csv(paste(path, 'cleandata2016.csv', sep = ""))
data2017 <- read_csv(paste(path, 'cleandata2017.csv', sep = ""))
data2018 <- read_csv(paste(path, 'cleandata2018.csv', sep = ""))
data2019 <- read_csv(paste(path, 'cleandata2019.csv', sep = ""))
#rename variables
data2016 <- rename(data2016, PATIENT_PROBLEM_CODE = PROBLEM_CODE)
data2019 <- rename(data2019, PATIENT_PROBLEM_DESCRIPTION = PATIENT_PROBLEM,
DEVICE_PROBLEM_DESCRIPTION = DEVICE_PROBLEM)
data2017 <- rename(data2017, PATIENT_PROBLEM_CODE = PROBLEM_CODE)
data2018 <- rename(data2018, PATIENT_PROBLEM_CODE = PROBLEM_CODE)
#get rid of first variable in data2018
data2018 <- data2018[,-c(1)]
#more data cleaning
data2017$DATE_OF_EVENT <- format(as.Date(as.character(data2017$DATE_OF_EVENT)), format = "%m/%d/%Y")
data2017$DATE_RECEIVED <- format(as.Date(as.character(data2017$DATE_RECEIVED)), format = "%m/%d/%Y")
data2019$DATE_OF_EVENT <- format(as.Date(as.character(data2019$DATE_OF_EVENT)), format = "%m/%d/%Y")
data2019$DATE_RECEIVED <- format(as.Date(as.character(data2019$DATE_RECEIVED)), format = "%m/%d/%Y")
#vertical merge
cleandata <- rbind(data2016, data2017, data2018, data2019)
